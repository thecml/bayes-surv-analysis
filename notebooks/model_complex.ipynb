{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "matplotlib_style = 'fivethirtyeight'\n",
    "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
    "\n",
    "from tools.baysurv_trainer import Trainer\n",
    "from utility.config import load_config\n",
    "from utility.training import get_data_loader, scale_data, split_time_event\n",
    "from tools.baysurv_builder import make_mlp_model, make_vi_model, make_mcd_model, make_sngp_model\n",
    "from utility.risk import InputFunction\n",
    "from utility.loss import CoxPHLoss, CoxPHLossGaussian\n",
    "from pathlib import Path\n",
    "import paths as pt\n",
    "from utility.survival import (calculate_event_times, calculate_percentiles, convert_to_structured,\n",
    "                              compute_deterministic_survival_curve, compute_nondeterministic_survival_curve)\n",
    "from utility.training import make_stratified_split\n",
    "from time import time\n",
    "from tools.evaluator import LifelinesEvaluator\n",
    "from pycox.evaluation import EvalSurv\n",
    "import math\n",
    "from utility.survival import coverage\n",
    "from scipy.stats import chisquare\n",
    "import torch\n",
    "from utility.survival import survival_probability_calibration\n",
    "from tools.Evaluations.util import make_monotonic, check_monotonicity\n",
    "from utility.survival import make_time_bins\n",
    "from utility.loss import cox_nll_tf\n",
    "\n",
    "class _TFColor(object):\n",
    "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
    "    red = '#F15854'\n",
    "    blue = '#5DA5DA'\n",
    "    orange = '#FAA43A'\n",
    "    green = '#60BD68'\n",
    "    pink = '#F17CB0'\n",
    "    brown = '#B2912F'\n",
    "    purple = '#B276B2'\n",
    "    yellow = '#DECF3F'\n",
    "    gray = '#4D4D4D'\n",
    "    def __getitem__(self, i):\n",
    "        return [\n",
    "            self.red,\n",
    "            self.orange,\n",
    "            self.green,\n",
    "            self.blue,\n",
    "            self.pink,\n",
    "            self.brown,\n",
    "            self.purple,\n",
    "            self.yellow,\n",
    "            self.gray,\n",
    "        ][i % 9]\n",
    "TFColor = _TFColor()\n",
    "\n",
    "N_SAMPLES_TRAIN = 10\n",
    "N_SAMPLES_TEST = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\au475271\\Miniconda3\\envs\\py39-baysurv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mlp - #Params: 2849\n",
      "Model name: sngp - #Params: 3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\au475271\\Miniconda3\\envs\\py39-baysurv\\lib\\site-packages\\tensorflow_probability\\python\\layers\\util.py:95: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  loc = add_variable_fn(\n",
      "c:\\Users\\au475271\\Miniconda3\\envs\\py39-baysurv\\lib\\site-packages\\tensorflow_probability\\python\\layers\\util.py:105: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  untransformed_scale = add_variable_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: vi - #Params: 5700\n",
      "Model name: mcd1 - #Params: 2882\n",
      "Model name: mcd2 - #Params: 2882\n",
      "Model name: mcd3 - #Params: 2882\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"MIMIC\"\n",
    "\n",
    "# Load training parameters\n",
    "config = load_config(pt.MLP_CONFIGS_DIR, f\"{dataset_name.lower()}.yaml\")\n",
    "optimizer = tf.keras.optimizers.deserialize(config['optimizer'])\n",
    "activation_fn = config['activiation_fn']\n",
    "layers = config['network_layers']\n",
    "l2_reg = config['l2_reg']\n",
    "batch_size = config['batch_size']\n",
    "early_stop = config['early_stop']\n",
    "patience = config['patience']\n",
    "n_samples_train = config['n_samples_train']\n",
    "n_samples_valid = config['n_samples_valid']\n",
    "n_samples_test = config['n_samples_test']\n",
    "\n",
    "# Load data\n",
    "dl = get_data_loader(dataset_name).load_data()\n",
    "num_features, cat_features = dl.get_features()\n",
    "df = dl.get_data()\n",
    "\n",
    "# Split data\n",
    "df_train, df_valid, df_test = make_stratified_split(df, stratify_colname='both', frac_train=0.7,\n",
    "                                                frac_valid=0.1, frac_test=0.2, random_state=0)\n",
    "X_train = df_train[cat_features+num_features]\n",
    "X_valid = df_valid[cat_features+num_features]\n",
    "X_test = df_test[cat_features+num_features]\n",
    "y_train = convert_to_structured(df_train[\"time\"], df_train[\"event\"])\n",
    "y_valid = convert_to_structured(df_valid[\"time\"], df_valid[\"event\"])\n",
    "y_test = convert_to_structured(df_test[\"time\"], df_test[\"event\"])\n",
    "\n",
    "# Scale data\n",
    "X_train, X_valid, X_test = scale_data(X_train, X_valid, X_test, cat_features, num_features)\n",
    "\n",
    "# Convert to array\n",
    "X_train = np.array(X_train)\n",
    "X_valid = np.array(X_valid)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Make time/event split\n",
    "t_train, e_train = split_time_event(y_train)\n",
    "t_valid, e_valid = split_time_event(y_valid)\n",
    "t_test, e_test = split_time_event(y_test)\n",
    "\n",
    "# Make event times\n",
    "time_bins = make_time_bins(t_train, event=e_train)\n",
    "\n",
    "# Calculate quantiles\n",
    "event_times_pct = calculate_percentiles(time_bins)\n",
    "\n",
    "# Make data loaders\n",
    "train_ds = InputFunction(X_train, t_train, e_train, batch_size=batch_size, drop_last=True, shuffle=True)()\n",
    "valid_ds = InputFunction(X_valid, t_valid, e_valid, batch_size=batch_size, shuffle=True)() # shuffle=True to avoid NaNs\n",
    "test_ds = InputFunction(X_test, t_test, e_test, batch_size=batch_size)()\n",
    "\n",
    "# Make models\n",
    "models = [\"mlp\", \"sngp\", \"vi\", \"mcd1\", \"mcd2\", \"mcd3\"]\n",
    "for model_name in models:\n",
    "    if model_name == \"mlp\":\n",
    "        dropout_rate = config['dropout_rate']\n",
    "        model = make_mlp_model(input_shape=X_train.shape[1:], output_dim=1,\n",
    "                                layers=layers, activation_fn=activation_fn,\n",
    "                                dropout_rate=dropout_rate, regularization_pen=l2_reg)\n",
    "    elif model_name == \"sngp\":\n",
    "        dropout_rate = config['dropout_rate']\n",
    "        model = make_sngp_model(input_shape=X_train.shape[1:], output_dim=1,\n",
    "                                layers=layers, activation_fn=activation_fn,\n",
    "                                dropout_rate=dropout_rate, regularization_pen=l2_reg)\n",
    "    elif model_name == \"vi\":\n",
    "        dropout_rate = config['dropout_rate']\n",
    "        model = make_vi_model(n_train_samples=len(X_train),\n",
    "                                input_shape=X_train.shape[1:], output_dim=2,\n",
    "                                layers=layers, activation_fn=activation_fn,\n",
    "                                dropout_rate=dropout_rate, regularization_pen=l2_reg)\n",
    "    elif model_name == \"mcd1\":\n",
    "        dropout_rate = 0.1\n",
    "        model = make_mcd_model(input_shape=X_train.shape[1:], output_dim=2,\n",
    "                                layers=layers, activation_fn=activation_fn,\n",
    "                                dropout_rate=dropout_rate, regularization_pen=l2_reg)\n",
    "    elif model_name == \"mcd2\":\n",
    "        dropout_rate = 0.2\n",
    "        model = make_mcd_model(input_shape=X_train.shape[1:], output_dim=2,\n",
    "                                layers=layers, activation_fn=activation_fn,\n",
    "                                dropout_rate=dropout_rate, regularization_pen=l2_reg)\n",
    "    elif model_name == \"mcd3\":\n",
    "        dropout_rate = 0.5\n",
    "        model = make_mcd_model(input_shape=X_train.shape[1:], output_dim=2,\n",
    "                                layers=layers, activation_fn=activation_fn,\n",
    "                                dropout_rate=dropout_rate, regularization_pen=l2_reg)\n",
    "    else:\n",
    "        raise ValueError(\"Model not found\")\n",
    "    \n",
    "    n_params = np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "    print(f\"Model name: {model_name} - #Params: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10718\n"
     ]
    }
   ],
   "source": [
    "from tools.sota_builder import make_baymtlr_model, make_baycox_model\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "config = dotdict(load_config(pt.BAYMTLR_CONFIGS_DIR, f\"{dataset_name.lower()}.yaml\"))\n",
    "config['hidden_size'] = 32\n",
    "model = make_baymtlr_model(num_features, time_bins, config)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5570\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(load_config(pt.BAYCOX_CONFIGS_DIR, f\"{dataset_name.lower()}.yaml\"))\n",
    "config['hidden_size'] = 32\n",
    "model = make_baycox_model(num_features, config)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baysurv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
